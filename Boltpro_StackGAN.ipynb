{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1EpUMQ6V94NANiEe2W6KuR6HvQpXp-E86",
      "authorship_tag": "ABX9TyPkzQrcZNbQ+OpZ0IIEBzIs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavankumarallu/Bolt-PRO/blob/main/Boltpro_StackGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DiZbdqgkWpxZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "import sys, os, math, argparse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model, Sequential\n",
        "from keras.layers import GRU, LSTM\n",
        "from keras.layers import Input, Dropout, Dense, Reshape, Flatten, Activation\n",
        "# from keras.layers.merge import _Merge\n",
        "from keras.layers import concatenate\n",
        "from keras.layers import Conv1D, Conv2D\n",
        "from keras.layers.convolutional import Convolution2D, Conv2DTranspose, UpSampling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.layers import Activation, ZeroPadding2D\n",
        "from keras.layers import TimeDistributed, RepeatVector\n",
        "# from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import LeakyReLU\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence as ksq\n",
        "from keras import backend as K\n",
        "from functools import partial\n",
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "2xtrZuYxWuhx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Sequence_HSepians.csv')"
      ],
      "metadata": {
        "id": "pGUm3YX0W3us"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CmFqlIgoXntB",
        "outputId": "004e19b3-6202-4579-baf0-e5229f061ff7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        simple_fasta\n",
              "0  MSLIQKEAQGQSGTDQTVVVLSNPTYYMSNDIPYTFHQDNNFLYLC...\n",
              "1  MQRDHTMDYKESCPSVSIPSSDEHREKKKRFTVYKVLVSVGRSEWF...\n",
              "2  MARLSGTVGVAAVTAGPGLTNTVTAVKNAQMAQSPILLLGGAASTL...\n",
              "3  MATSLDFKTYVDQACRAAEEFVNIYYETMDKRRRALTRLYLDKATL...\n",
              "4  MVFRRFVEVGRVAYVSFGPHAGKLVAIVDVIDQNRALVDGPCTQVR..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ab39c4a2-9307-4bce-a51e-a16c2e6dc9b2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>simple_fasta</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MSLIQKEAQGQSGTDQTVVVLSNPTYYMSNDIPYTFHQDNNFLYLC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MQRDHTMDYKESCPSVSIPSSDEHREKKKRFTVYKVLVSVGRSEWF...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>MARLSGTVGVAAVTAGPGLTNTVTAVKNAQMAQSPILLLGGAASTL...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MATSLDFKTYVDQACRAAEEFVNIYYETMDKRRRALTRLYLDKATL...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MVFRRFVEVGRVAYVSFGPHAGKLVAIVDVIDQNRALVDGPCTQVR...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab39c4a2-9307-4bce-a51e-a16c2e6dc9b2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ab39c4a2-9307-4bce-a51e-a16c2e6dc9b2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ab39c4a2-9307-4bce-a51e-a16c2e6dc9b2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import text, sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences"
      ],
      "metadata": {
        "id": "BCBSifrfXrdP"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# maximum length of sequence, everything afterwards is discarded!\n",
        "max_length = 400\n",
        "\n",
        "#create and fit tokenizer\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(data['simple_fasta'])\n",
        "#represent input data as word rank number sequences\n",
        "train_X = tokenizer.texts_to_sequences(data['simple_fasta'])\n",
        "train_X = pd.DataFrame(pad_sequences(train_X, maxlen=max_length))"
      ],
      "metadata": {
        "id": "WRfP9fIEYAE6"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "QS7-B265YNo0",
        "outputId": "cfc26f84-a089-420f-e063-99f2eb7fa713"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   0    1    2    3    4    5    6    7    8    9    ...  390  391  392  393  \\\n",
              "0    3   14   10   13    8   16   11   15   17   12  ...   10   13    2   12   \n",
              "1    0    0    0    0    0    0    0    0    0    0  ...   18   10    3    8   \n",
              "2    0    0    0    0    0    0    0    0    0    0  ...    1   10   13    8   \n",
              "3    0    0    0    0    0    0    0    0    0    0  ...   19   15    9   15   \n",
              "4    0    0    0    0    0    0    0    0    0    0  ...    4    5    5   18   \n",
              "\n",
              "   394  395  396  397  398  399  \n",
              "0   13   19    3   12    4    3  \n",
              "1    5   17   12    3   10    8  \n",
              "2   12    4    3    8   12   12  \n",
              "3   12   10   20    3    3    3  \n",
              "4    2   12   14   14   12    2  \n",
              "\n",
              "[5 rows x 400 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8425e648-e02f-4b83-9e4b-f04c8900f7ca\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>390</th>\n",
              "      <th>391</th>\n",
              "      <th>392</th>\n",
              "      <th>393</th>\n",
              "      <th>394</th>\n",
              "      <th>395</th>\n",
              "      <th>396</th>\n",
              "      <th>397</th>\n",
              "      <th>398</th>\n",
              "      <th>399</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>10</td>\n",
              "      <td>13</td>\n",
              "      <td>8</td>\n",
              "      <td>16</td>\n",
              "      <td>11</td>\n",
              "      <td>15</td>\n",
              "      <td>17</td>\n",
              "      <td>12</td>\n",
              "      <td>...</td>\n",
              "      <td>10</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>13</td>\n",
              "      <td>19</td>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>17</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>13</td>\n",
              "      <td>8</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>9</td>\n",
              "      <td>15</td>\n",
              "      <td>12</td>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 400 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8425e648-e02f-4b83-9e4b-f04c8900f7ca')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8425e648-e02f-4b83-9e4b-f04c8900f7ca button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8425e648-e02f-4b83-9e4b-f04c8900f7ca');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max(train_X.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJajucf_YkxT",
        "outputId": "bc935838-655b-4042-cfda-f4c7e62ca9f3"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class proteinGAN():\n",
        "    def __init__(self, n_aa=max_length, n_render=1, latent_dim=10):\n",
        "        self.n_aa = n_aa\n",
        "        self.n_render = n_render\n",
        "        self.pro_shape = (self.n_aa, self.n_render)\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "        # Generated peptide\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        peptide = self.generator(z)\n",
        "        # The discriminator takes generated peptides as input and determines validity\n",
        "        validity = self.discriminator(peptide)\n",
        "        # For the combined model we will only train the generator\n",
        "        for layer in self.generator.layers:\n",
        "            layer.trainable=True\n",
        "        for layer in self.discriminator.layers:\n",
        "            layer.trainable=False\n",
        "        self.generator.trainable=True\n",
        "        self.discriminator.trainable = False\n",
        "        self.combined = Model(z, validity)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "        self.combined.summary()\n",
        "        for layer in self.generator.layers:\n",
        "            layer.trainable=False\n",
        "        for layer in self.discriminator.layers:\n",
        "            layer.trainable=True\n",
        "        self.generator.trainable=False\n",
        "        self.discriminator.trainable = True\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "    \n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(256, input_shape=(self.latent_dim,)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(1024))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(np.prod(self.pro_shape), activation='tanh'))\n",
        "        model.add(Reshape(self.pro_shape))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        peptide = model(noise)\n",
        "\n",
        "        return Model(noise, peptide)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Flatten(input_shape=self.pro_shape))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(256))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.summary()\n",
        "\n",
        "        peptide = Input(shape=self.pro_shape)\n",
        "        validity = model(peptide)\n",
        "        return Model(peptide, validity)\n",
        "    \n",
        "    def train(self, X, epochs, batch_size=128, sample_interval=50):\n",
        "        # Load the dataset\n",
        "        X_train = X\n",
        "\n",
        "        # Rescale\n",
        "        X_train = X_train / 20.0\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            peptides = X_train[idx]\n",
        "\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim ))\n",
        "\n",
        "            # Generate a new batch\n",
        "            gen_peptides = self.generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator\n",
        "            d_loss_real = self.discriminator.train_on_batch(peptides, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_peptides, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim ))\n",
        "\n",
        "            # Train the generator (to have the discriminator label samples as valid)\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "\n",
        "            # Plot the progress\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "            # If at save interval\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_peptides(epoch)\n",
        "            \n",
        "        return self.combined\n",
        "    \n",
        "    def sample_peptides(self, epoch):\n",
        "        noise = np.random.normal(0, 1, (1, self.latent_dim))\n",
        "        gen_peptides = self.generator.predict(noise)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_peptides = 20.0 * gen_peptides\n",
        "        return gen_peptides"
      ],
      "metadata": {
        "id": "rk9wxZOuYn1A"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.normal(0,1,5).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_Q62GZEYrVr",
        "outputId": "1f03c3a6-4957-4355-ed20-bc4d477c1f5f"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5,)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pro_gan = proteinGAN()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfsqekLTYtpg",
        "outputId": "106e0c62-db79-4436-b43b-7232dc03663e"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_3 (Flatten)         (None, 400)               0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 512)               205312    \n",
            "                                                                 \n",
            " leaky_re_lu_15 (LeakyReLU)  (None, 512)               0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " leaky_re_lu_16 (LeakyReLU)  (None, 256)               0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 336,897\n",
            "Trainable params: 336,897\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_24 (Dense)            (None, 256)               2816      \n",
            "                                                                 \n",
            " leaky_re_lu_17 (LeakyReLU)  (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 512)               131584    \n",
            "                                                                 \n",
            " leaky_re_lu_18 (LeakyReLU)  (None, 512)               0         \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 512)              2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 1024)              525312    \n",
            "                                                                 \n",
            " leaky_re_lu_19 (LeakyReLU)  (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 1024)             4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 400)               410000    \n",
            "                                                                 \n",
            " reshape_3 (Reshape)         (None, 400, 1)            0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,076,880\n",
            "Trainable params: 1,073,296\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n",
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_12 (InputLayer)       [(None, 10)]              0         \n",
            "                                                                 \n",
            " model_10 (Functional)       (None, 400, 1)            1076880   \n",
            "                                                                 \n",
            " model_9 (Functional)        (None, 1)                 336897    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,413,777\n",
            "Trainable params: 1,073,296\n",
            "Non-trainable params: 340,481\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X.values.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLzeZyeIYv2n",
        "outputId": "45fbc2ae-e4ca-42c9-a614-36a80f76150c"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6456, 400)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X_3 = np.reshape(train_X.values, (train_X.shape[0], train_X.shape[1], 1))"
      ],
      "metadata": {
        "id": "LsdKi8g3YyL3"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pro_gan.train(X=train_X_3,  epochs=300, batch_size=32, sample_interval=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gAghhaYY0Iw",
        "outputId": "67cffbaa-a514-4461-917a-3fd6174fc945"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 166ms/step\n",
            "0 [D loss: 0.559327, acc.: 57.81%] [G loss: 0.602466]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1 [D loss: 0.471426, acc.: 53.12%] [G loss: 0.531420]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2 [D loss: 0.444248, acc.: 50.00%] [G loss: 0.446733]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "3 [D loss: 0.415827, acc.: 54.69%] [G loss: 0.373658]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "4 [D loss: 0.354211, acc.: 96.88%] [G loss: 0.397156]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "5 [D loss: 0.260465, acc.: 100.00%] [G loss: 0.377389]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "6 [D loss: 0.199350, acc.: 100.00%] [G loss: 0.381437]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "7 [D loss: 0.217494, acc.: 100.00%] [G loss: 0.362095]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "8 [D loss: 0.154608, acc.: 100.00%] [G loss: 0.387876]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "9 [D loss: 0.136955, acc.: 100.00%] [G loss: 0.514702]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "10 [D loss: 0.169159, acc.: 100.00%] [G loss: 0.488575]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "11 [D loss: 0.175530, acc.: 100.00%] [G loss: 0.521162]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "12 [D loss: 0.192212, acc.: 98.44%] [G loss: 0.721539]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "13 [D loss: 0.181732, acc.: 98.44%] [G loss: 0.967330]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "14 [D loss: 0.184510, acc.: 98.44%] [G loss: 1.116003]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "15 [D loss: 0.192848, acc.: 98.44%] [G loss: 1.471613]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "16 [D loss: 0.183676, acc.: 96.88%] [G loss: 1.698413]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "17 [D loss: 0.139417, acc.: 100.00%] [G loss: 1.969699]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "18 [D loss: 0.155092, acc.: 100.00%] [G loss: 2.146718]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "19 [D loss: 0.164612, acc.: 93.75%] [G loss: 2.432098]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "20 [D loss: 0.132242, acc.: 100.00%] [G loss: 2.452364]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "21 [D loss: 0.128050, acc.: 98.44%] [G loss: 2.801505]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "22 [D loss: 0.110360, acc.: 100.00%] [G loss: 2.894801]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "23 [D loss: 0.106128, acc.: 98.44%] [G loss: 3.001678]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "24 [D loss: 0.088758, acc.: 98.44%] [G loss: 3.015124]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "25 [D loss: 0.103655, acc.: 98.44%] [G loss: 2.974559]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "26 [D loss: 0.118614, acc.: 100.00%] [G loss: 3.010440]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "27 [D loss: 0.101719, acc.: 98.44%] [G loss: 3.445217]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "28 [D loss: 0.100954, acc.: 98.44%] [G loss: 3.421767]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "29 [D loss: 0.095341, acc.: 96.88%] [G loss: 3.718382]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "30 [D loss: 0.134458, acc.: 96.88%] [G loss: 3.704348]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "31 [D loss: 0.126481, acc.: 95.31%] [G loss: 3.786663]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "32 [D loss: 0.065640, acc.: 100.00%] [G loss: 3.895621]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "33 [D loss: 0.082531, acc.: 100.00%] [G loss: 3.454283]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "34 [D loss: 0.072972, acc.: 100.00%] [G loss: 3.745652]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "35 [D loss: 0.081507, acc.: 100.00%] [G loss: 3.579776]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "36 [D loss: 0.058751, acc.: 100.00%] [G loss: 3.519872]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "37 [D loss: 0.124167, acc.: 95.31%] [G loss: 3.427127]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "38 [D loss: 0.136308, acc.: 93.75%] [G loss: 3.548133]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "39 [D loss: 0.072548, acc.: 98.44%] [G loss: 4.011472]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "40 [D loss: 0.094869, acc.: 98.44%] [G loss: 3.802862]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "41 [D loss: 0.098751, acc.: 98.44%] [G loss: 3.671342]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "42 [D loss: 0.147833, acc.: 93.75%] [G loss: 4.138850]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "43 [D loss: 0.154514, acc.: 93.75%] [G loss: 3.733660]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "44 [D loss: 0.155744, acc.: 92.19%] [G loss: 3.994461]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "45 [D loss: 0.181072, acc.: 90.62%] [G loss: 4.053841]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "46 [D loss: 0.153611, acc.: 92.19%] [G loss: 4.054229]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "47 [D loss: 0.221310, acc.: 93.75%] [G loss: 4.129956]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "48 [D loss: 0.162407, acc.: 95.31%] [G loss: 4.179121]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "49 [D loss: 0.173009, acc.: 93.75%] [G loss: 4.157121]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "50 [D loss: 0.149437, acc.: 93.75%] [G loss: 3.924448]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "51 [D loss: 0.151619, acc.: 95.31%] [G loss: 3.601818]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "52 [D loss: 0.299666, acc.: 92.19%] [G loss: 4.007403]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "53 [D loss: 0.157209, acc.: 95.31%] [G loss: 4.058317]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "54 [D loss: 0.206656, acc.: 93.75%] [G loss: 4.065947]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "55 [D loss: 0.261235, acc.: 90.62%] [G loss: 3.813620]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "56 [D loss: 0.278072, acc.: 89.06%] [G loss: 4.525537]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "57 [D loss: 0.344182, acc.: 82.81%] [G loss: 4.374666]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "58 [D loss: 0.173166, acc.: 93.75%] [G loss: 3.542661]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "59 [D loss: 0.088837, acc.: 98.44%] [G loss: 3.343737]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "60 [D loss: 0.121170, acc.: 96.88%] [G loss: 3.694793]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "61 [D loss: 0.184109, acc.: 92.19%] [G loss: 3.577244]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "62 [D loss: 0.238819, acc.: 92.19%] [G loss: 4.157526]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "63 [D loss: 0.147146, acc.: 95.31%] [G loss: 3.680154]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "64 [D loss: 0.254506, acc.: 90.62%] [G loss: 3.746773]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "65 [D loss: 0.259990, acc.: 92.19%] [G loss: 3.777348]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "66 [D loss: 0.295908, acc.: 93.75%] [G loss: 3.844921]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "67 [D loss: 0.409282, acc.: 89.06%] [G loss: 4.028039]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "68 [D loss: 0.490771, acc.: 81.25%] [G loss: 4.081891]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "69 [D loss: 0.255238, acc.: 95.31%] [G loss: 3.914434]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "70 [D loss: 0.216753, acc.: 95.31%] [G loss: 3.486708]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "71 [D loss: 0.341453, acc.: 84.38%] [G loss: 3.875570]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "72 [D loss: 0.372294, acc.: 85.94%] [G loss: 3.753744]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "73 [D loss: 0.363150, acc.: 87.50%] [G loss: 3.151614]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "74 [D loss: 0.332017, acc.: 85.94%] [G loss: 3.432755]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "75 [D loss: 0.280306, acc.: 95.31%] [G loss: 3.244862]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "76 [D loss: 0.350049, acc.: 85.94%] [G loss: 3.535186]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "77 [D loss: 0.299056, acc.: 89.06%] [G loss: 3.217731]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "78 [D loss: 0.220644, acc.: 90.62%] [G loss: 3.107328]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "79 [D loss: 0.421846, acc.: 85.94%] [G loss: 3.576564]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "80 [D loss: 0.223807, acc.: 95.31%] [G loss: 3.534113]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "81 [D loss: 0.348849, acc.: 85.94%] [G loss: 3.625520]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "82 [D loss: 0.308359, acc.: 90.62%] [G loss: 3.237814]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "83 [D loss: 0.217956, acc.: 93.75%] [G loss: 3.427391]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "84 [D loss: 0.247035, acc.: 95.31%] [G loss: 3.195008]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "85 [D loss: 0.374812, acc.: 84.38%] [G loss: 3.116901]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "86 [D loss: 0.225013, acc.: 93.75%] [G loss: 3.264040]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "87 [D loss: 0.329522, acc.: 90.62%] [G loss: 3.019634]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "88 [D loss: 0.231123, acc.: 93.75%] [G loss: 3.039430]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "89 [D loss: 0.469641, acc.: 79.69%] [G loss: 3.258245]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "90 [D loss: 0.355219, acc.: 82.81%] [G loss: 3.386722]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "91 [D loss: 0.407414, acc.: 84.38%] [G loss: 2.886693]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "92 [D loss: 0.505811, acc.: 82.81%] [G loss: 3.520280]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "93 [D loss: 0.383843, acc.: 87.50%] [G loss: 3.161191]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "94 [D loss: 0.389877, acc.: 89.06%] [G loss: 2.885667]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "95 [D loss: 0.376301, acc.: 89.06%] [G loss: 3.485600]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "96 [D loss: 0.451378, acc.: 79.69%] [G loss: 3.360462]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "97 [D loss: 0.381706, acc.: 85.94%] [G loss: 3.456980]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "98 [D loss: 0.554048, acc.: 79.69%] [G loss: 3.215706]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "99 [D loss: 0.496714, acc.: 75.00%] [G loss: 3.278555]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "100 [D loss: 0.418653, acc.: 81.25%] [G loss: 3.147562]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "101 [D loss: 0.353508, acc.: 89.06%] [G loss: 3.309509]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "102 [D loss: 0.416496, acc.: 85.94%] [G loss: 3.381036]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "103 [D loss: 0.297716, acc.: 90.62%] [G loss: 2.950385]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "104 [D loss: 0.220823, acc.: 93.75%] [G loss: 2.655579]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "105 [D loss: 0.356545, acc.: 85.94%] [G loss: 2.758804]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "106 [D loss: 0.324853, acc.: 89.06%] [G loss: 3.349087]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "107 [D loss: 0.323701, acc.: 92.19%] [G loss: 2.671801]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "108 [D loss: 0.333882, acc.: 84.38%] [G loss: 3.039422]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "109 [D loss: 0.442019, acc.: 82.81%] [G loss: 3.049711]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "110 [D loss: 0.342215, acc.: 87.50%] [G loss: 2.961695]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "111 [D loss: 0.399039, acc.: 87.50%] [G loss: 2.856314]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "112 [D loss: 0.374188, acc.: 87.50%] [G loss: 3.220342]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "113 [D loss: 0.233637, acc.: 95.31%] [G loss: 2.777138]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "114 [D loss: 0.406630, acc.: 82.81%] [G loss: 2.488353]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "115 [D loss: 0.368142, acc.: 89.06%] [G loss: 2.887588]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "116 [D loss: 0.450686, acc.: 82.81%] [G loss: 2.910521]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "117 [D loss: 0.406367, acc.: 85.94%] [G loss: 2.601851]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "118 [D loss: 0.311917, acc.: 89.06%] [G loss: 2.818349]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "119 [D loss: 0.342855, acc.: 89.06%] [G loss: 2.733644]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "120 [D loss: 0.240388, acc.: 93.75%] [G loss: 2.833838]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "121 [D loss: 0.347336, acc.: 87.50%] [G loss: 2.851924]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "122 [D loss: 0.358199, acc.: 89.06%] [G loss: 2.781513]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "123 [D loss: 0.350972, acc.: 85.94%] [G loss: 2.983505]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "124 [D loss: 0.415570, acc.: 87.50%] [G loss: 3.027213]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "125 [D loss: 0.375258, acc.: 84.38%] [G loss: 2.864915]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "126 [D loss: 0.350313, acc.: 85.94%] [G loss: 2.605279]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "127 [D loss: 0.314272, acc.: 90.62%] [G loss: 2.879573]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "128 [D loss: 0.448936, acc.: 79.69%] [G loss: 2.980808]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "129 [D loss: 0.431062, acc.: 82.81%] [G loss: 2.790319]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "130 [D loss: 0.533899, acc.: 81.25%] [G loss: 3.070189]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "131 [D loss: 0.285849, acc.: 90.62%] [G loss: 2.604863]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "132 [D loss: 0.294776, acc.: 85.94%] [G loss: 2.688952]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "133 [D loss: 0.308050, acc.: 90.62%] [G loss: 2.390703]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "134 [D loss: 0.365778, acc.: 82.81%] [G loss: 2.167889]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "135 [D loss: 0.380012, acc.: 81.25%] [G loss: 2.906657]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "136 [D loss: 0.410042, acc.: 87.50%] [G loss: 2.681834]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "137 [D loss: 0.415490, acc.: 81.25%] [G loss: 2.614781]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "138 [D loss: 0.400921, acc.: 87.50%] [G loss: 2.579500]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "139 [D loss: 0.317912, acc.: 90.62%] [G loss: 2.633595]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "140 [D loss: 0.386953, acc.: 85.94%] [G loss: 2.706655]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "141 [D loss: 0.367386, acc.: 89.06%] [G loss: 2.729932]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "142 [D loss: 0.370492, acc.: 85.94%] [G loss: 2.724800]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "143 [D loss: 0.382088, acc.: 87.50%] [G loss: 2.563346]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "144 [D loss: 0.343245, acc.: 87.50%] [G loss: 2.518685]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "145 [D loss: 0.389856, acc.: 84.38%] [G loss: 2.712574]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "146 [D loss: 0.353071, acc.: 89.06%] [G loss: 2.733267]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "147 [D loss: 0.343245, acc.: 90.62%] [G loss: 2.502880]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "148 [D loss: 0.495137, acc.: 81.25%] [G loss: 2.865140]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "149 [D loss: 0.419267, acc.: 84.38%] [G loss: 2.577557]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "150 [D loss: 0.370608, acc.: 87.50%] [G loss: 2.605994]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "151 [D loss: 0.504581, acc.: 76.56%] [G loss: 2.451832]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "152 [D loss: 0.514775, acc.: 82.81%] [G loss: 2.653488]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "153 [D loss: 0.587900, acc.: 73.44%] [G loss: 2.547802]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "154 [D loss: 0.564879, acc.: 75.00%] [G loss: 2.320580]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "155 [D loss: 0.465410, acc.: 79.69%] [G loss: 2.356114]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "156 [D loss: 0.492924, acc.: 76.56%] [G loss: 2.039439]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "157 [D loss: 0.533504, acc.: 81.25%] [G loss: 2.734800]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "158 [D loss: 0.563146, acc.: 75.00%] [G loss: 2.610492]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "159 [D loss: 0.535627, acc.: 78.12%] [G loss: 2.653532]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "160 [D loss: 0.576828, acc.: 76.56%] [G loss: 2.582038]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "161 [D loss: 0.426781, acc.: 81.25%] [G loss: 2.413050]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "162 [D loss: 0.477830, acc.: 84.38%] [G loss: 2.535392]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "163 [D loss: 0.466244, acc.: 85.94%] [G loss: 2.386407]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "164 [D loss: 0.406985, acc.: 84.38%] [G loss: 2.276542]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "165 [D loss: 0.359321, acc.: 87.50%] [G loss: 2.187608]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "166 [D loss: 0.472031, acc.: 76.56%] [G loss: 2.003272]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "167 [D loss: 0.404626, acc.: 85.94%] [G loss: 2.244952]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "168 [D loss: 0.363152, acc.: 87.50%] [G loss: 2.114099]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "169 [D loss: 0.372543, acc.: 92.19%] [G loss: 2.382510]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "170 [D loss: 0.430420, acc.: 84.38%] [G loss: 2.431011]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "171 [D loss: 0.391913, acc.: 85.94%] [G loss: 2.828699]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "172 [D loss: 0.416582, acc.: 85.94%] [G loss: 2.161256]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "173 [D loss: 0.382551, acc.: 89.06%] [G loss: 2.091005]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "174 [D loss: 0.404123, acc.: 87.50%] [G loss: 2.160421]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "175 [D loss: 0.469598, acc.: 78.12%] [G loss: 2.428648]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "176 [D loss: 0.378243, acc.: 87.50%] [G loss: 2.110565]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "177 [D loss: 0.489320, acc.: 76.56%] [G loss: 2.142246]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "178 [D loss: 0.486694, acc.: 79.69%] [G loss: 3.006174]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "179 [D loss: 0.445871, acc.: 81.25%] [G loss: 1.910375]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "180 [D loss: 0.462850, acc.: 79.69%] [G loss: 2.273170]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "181 [D loss: 0.433438, acc.: 82.81%] [G loss: 2.664032]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "182 [D loss: 0.468389, acc.: 78.12%] [G loss: 2.051896]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "183 [D loss: 0.382645, acc.: 89.06%] [G loss: 1.773566]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "184 [D loss: 0.392962, acc.: 89.06%] [G loss: 1.986889]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "185 [D loss: 0.363920, acc.: 87.50%] [G loss: 2.105947]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "186 [D loss: 0.512506, acc.: 75.00%] [G loss: 2.265431]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "187 [D loss: 0.539494, acc.: 71.88%] [G loss: 2.516243]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "188 [D loss: 0.534296, acc.: 76.56%] [G loss: 2.596859]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "189 [D loss: 0.469265, acc.: 84.38%] [G loss: 2.455217]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "190 [D loss: 0.492102, acc.: 76.56%] [G loss: 2.032481]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "191 [D loss: 0.491782, acc.: 78.12%] [G loss: 2.144684]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "192 [D loss: 0.519694, acc.: 70.31%] [G loss: 2.334203]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "193 [D loss: 0.567491, acc.: 73.44%] [G loss: 2.171412]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "194 [D loss: 0.549340, acc.: 78.12%] [G loss: 2.142005]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "195 [D loss: 0.386173, acc.: 90.62%] [G loss: 2.363896]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "196 [D loss: 0.484391, acc.: 75.00%] [G loss: 2.660388]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "197 [D loss: 0.696501, acc.: 57.81%] [G loss: 2.218693]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "198 [D loss: 0.515613, acc.: 71.88%] [G loss: 1.688055]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "199 [D loss: 0.491180, acc.: 79.69%] [G loss: 2.206395]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "200 [D loss: 0.467705, acc.: 78.12%] [G loss: 1.903584]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "201 [D loss: 0.558815, acc.: 73.44%] [G loss: 1.884667]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "202 [D loss: 0.482340, acc.: 71.88%] [G loss: 1.994580]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "203 [D loss: 0.553858, acc.: 73.44%] [G loss: 2.501787]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "204 [D loss: 0.474843, acc.: 76.56%] [G loss: 2.459315]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "205 [D loss: 0.582099, acc.: 70.31%] [G loss: 2.087635]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "206 [D loss: 0.511681, acc.: 73.44%] [G loss: 2.424640]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "207 [D loss: 0.631446, acc.: 59.38%] [G loss: 2.024385]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "208 [D loss: 0.552115, acc.: 71.88%] [G loss: 1.577805]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "209 [D loss: 0.521056, acc.: 78.12%] [G loss: 1.841157]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "210 [D loss: 0.570775, acc.: 70.31%] [G loss: 2.232006]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "211 [D loss: 0.566524, acc.: 73.44%] [G loss: 1.873888]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "212 [D loss: 0.618862, acc.: 70.31%] [G loss: 1.849504]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "213 [D loss: 0.780385, acc.: 56.25%] [G loss: 1.884147]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "214 [D loss: 0.609587, acc.: 68.75%] [G loss: 2.257907]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "215 [D loss: 0.700069, acc.: 48.44%] [G loss: 1.766777]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "216 [D loss: 0.663566, acc.: 62.50%] [G loss: 1.926270]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "217 [D loss: 0.621564, acc.: 64.06%] [G loss: 2.185839]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "218 [D loss: 0.701680, acc.: 51.56%] [G loss: 1.716631]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "219 [D loss: 0.638369, acc.: 67.19%] [G loss: 1.690406]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "220 [D loss: 0.631207, acc.: 62.50%] [G loss: 1.556188]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "221 [D loss: 0.672195, acc.: 62.50%] [G loss: 1.879043]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "222 [D loss: 0.542692, acc.: 71.88%] [G loss: 1.656747]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "223 [D loss: 0.732241, acc.: 53.12%] [G loss: 1.878378]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "224 [D loss: 0.663314, acc.: 67.19%] [G loss: 1.779823]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "225 [D loss: 0.716559, acc.: 60.94%] [G loss: 1.812787]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "226 [D loss: 0.695016, acc.: 53.12%] [G loss: 1.783155]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "227 [D loss: 0.786710, acc.: 50.00%] [G loss: 1.840614]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "228 [D loss: 0.790405, acc.: 50.00%] [G loss: 1.869000]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "229 [D loss: 0.893097, acc.: 32.81%] [G loss: 1.550109]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "230 [D loss: 0.779119, acc.: 50.00%] [G loss: 1.324097]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "231 [D loss: 0.766857, acc.: 50.00%] [G loss: 1.448112]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "232 [D loss: 0.656562, acc.: 64.06%] [G loss: 1.752463]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "233 [D loss: 0.722647, acc.: 46.88%] [G loss: 1.687124]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "234 [D loss: 0.720111, acc.: 54.69%] [G loss: 1.755329]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "235 [D loss: 0.753331, acc.: 48.44%] [G loss: 1.252438]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "236 [D loss: 0.809707, acc.: 51.56%] [G loss: 1.300977]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "237 [D loss: 0.889920, acc.: 32.81%] [G loss: 1.593712]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "238 [D loss: 0.759825, acc.: 39.06%] [G loss: 1.675160]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "239 [D loss: 0.682169, acc.: 57.81%] [G loss: 1.748622]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "240 [D loss: 0.699761, acc.: 54.69%] [G loss: 1.253768]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "241 [D loss: 0.795506, acc.: 45.31%] [G loss: 1.333355]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "242 [D loss: 0.720458, acc.: 54.69%] [G loss: 1.256750]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "243 [D loss: 0.666581, acc.: 64.06%] [G loss: 1.271014]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "244 [D loss: 0.778911, acc.: 40.62%] [G loss: 1.277023]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "245 [D loss: 0.773852, acc.: 56.25%] [G loss: 1.140927]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "246 [D loss: 0.851097, acc.: 43.75%] [G loss: 1.408401]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "247 [D loss: 0.766089, acc.: 42.19%] [G loss: 1.446285]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "248 [D loss: 0.737577, acc.: 50.00%] [G loss: 1.359621]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "249 [D loss: 0.946780, acc.: 34.38%] [G loss: 1.271537]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "250 [D loss: 0.874256, acc.: 29.69%] [G loss: 1.222623]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "251 [D loss: 0.912483, acc.: 17.19%] [G loss: 1.101902]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "252 [D loss: 0.871186, acc.: 32.81%] [G loss: 1.106134]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "253 [D loss: 0.816079, acc.: 40.62%] [G loss: 1.016071]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "254 [D loss: 0.804272, acc.: 40.62%] [G loss: 1.140988]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "255 [D loss: 0.923302, acc.: 34.38%] [G loss: 1.296794]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "256 [D loss: 0.829240, acc.: 37.50%] [G loss: 1.042766]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "257 [D loss: 0.807987, acc.: 43.75%] [G loss: 1.065267]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "258 [D loss: 0.802688, acc.: 43.75%] [G loss: 1.134925]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "259 [D loss: 0.840156, acc.: 42.19%] [G loss: 1.092110]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "260 [D loss: 0.831376, acc.: 37.50%] [G loss: 1.170867]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "261 [D loss: 0.807635, acc.: 51.56%] [G loss: 1.045584]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "262 [D loss: 0.856135, acc.: 34.38%] [G loss: 1.000541]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "263 [D loss: 0.809203, acc.: 40.62%] [G loss: 1.046951]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "264 [D loss: 0.873122, acc.: 34.38%] [G loss: 1.043495]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "265 [D loss: 0.763593, acc.: 43.75%] [G loss: 1.053982]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "266 [D loss: 0.794233, acc.: 42.19%] [G loss: 1.080199]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "267 [D loss: 0.807033, acc.: 39.06%] [G loss: 1.210293]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "268 [D loss: 0.801600, acc.: 35.94%] [G loss: 1.038674]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "269 [D loss: 0.779064, acc.: 43.75%] [G loss: 1.022271]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "270 [D loss: 0.849669, acc.: 35.94%] [G loss: 1.001348]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "271 [D loss: 0.921613, acc.: 29.69%] [G loss: 0.971810]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "272 [D loss: 0.846837, acc.: 34.38%] [G loss: 1.055749]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "273 [D loss: 0.831223, acc.: 31.25%] [G loss: 1.031921]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "274 [D loss: 0.806365, acc.: 35.94%] [G loss: 1.047747]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "275 [D loss: 0.882953, acc.: 34.38%] [G loss: 0.951132]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "276 [D loss: 0.838354, acc.: 31.25%] [G loss: 0.854188]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "277 [D loss: 0.866578, acc.: 21.88%] [G loss: 0.912609]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "278 [D loss: 0.823419, acc.: 17.19%] [G loss: 0.910929]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "279 [D loss: 0.851488, acc.: 18.75%] [G loss: 0.917346]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "280 [D loss: 0.839413, acc.: 28.12%] [G loss: 1.045531]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "281 [D loss: 0.799924, acc.: 25.00%] [G loss: 0.979766]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "282 [D loss: 0.820111, acc.: 25.00%] [G loss: 0.852863]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "283 [D loss: 0.841007, acc.: 29.69%] [G loss: 0.997153]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "284 [D loss: 0.818528, acc.: 31.25%] [G loss: 0.962450]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "285 [D loss: 0.808722, acc.: 28.12%] [G loss: 0.883272]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "286 [D loss: 0.785810, acc.: 32.81%] [G loss: 0.865179]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "287 [D loss: 0.827670, acc.: 31.25%] [G loss: 0.910801]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "288 [D loss: 0.782149, acc.: 35.94%] [G loss: 0.960793]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "289 [D loss: 0.778675, acc.: 37.50%] [G loss: 0.950182]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "290 [D loss: 0.786193, acc.: 29.69%] [G loss: 0.945231]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "291 [D loss: 0.857967, acc.: 23.44%] [G loss: 0.917542]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "292 [D loss: 0.784291, acc.: 32.81%] [G loss: 0.896628]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "293 [D loss: 0.774460, acc.: 39.06%] [G loss: 0.963171]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "294 [D loss: 0.799825, acc.: 35.94%] [G loss: 0.941285]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "295 [D loss: 0.811070, acc.: 32.81%] [G loss: 0.958143]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "296 [D loss: 0.786783, acc.: 32.81%] [G loss: 0.920544]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "297 [D loss: 0.801689, acc.: 31.25%] [G loss: 0.904207]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "298 [D loss: 0.779768, acc.: 39.06%] [G loss: 0.921535]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "299 [D loss: 0.767493, acc.: 32.81%] [G loss: 0.901700]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.functional.Functional at 0x7fd6c65592e0>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_arr = list(np.abs(np.reshape(pro_gan.sample_peptides(1), (max_length,)).astype(np.int)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6e0U9ygY1TL",
        "outputId": "ca7d4211-7b47-4235-9ab4-d38150e4283c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-62-a052ce9ef0a0>:1: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  tmp_arr = list(np.abs(np.reshape(pro_gan.sample_peptides(1), (max_length,)).astype(np.int)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha=list('ARNDCQEGHILKMFPSTWYVOXUZB')"
      ],
      "metadata": {
        "id": "_ZFZcutkZFQp"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq = ''.join([alpha[id] for id in tmp_arr])"
      ],
      "metadata": {
        "id": "DBcrHT65ZIKC"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(tmp_arr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uFhrE8ycz9S",
        "outputId": "24793dea-f212-4475-d16a-6e5999d51502"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUoHk94WZJwr",
        "outputId": "6969b83f-e4fb-4378-e37c-18d6d2290a2a"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ran_seq = \"CAHBHBKCICAKMNKHEEDOCNRDEFEHOBOMENEIGHGGDBCCNFCNLKFNMBECAKEDQHGPELAKLECADNLCDBENCNCOOGOFGEOAFFLCPBLDJMFPIJLHCMFAAADEAKDGCMFPJBBFEEEFFDCNQAFAGCJBIHDGQFAAIGFJDMGEHCCDJFFJIDHCBHCLGAKKFNIIBLFAFGHMIHLEIJCNKANHHBKCIAOFLODIHMDKDCJKRCELRCAPBLKFAQKDBBBQCFENDTJRHDHOGAMCGIEECDKDGAACKGFHDCEIEGBEPCEHCEECADJFMBFKMEFPJFEINAELMLKEBBCHIQLDAJHKDFCJCDKFLEALDFHMSFMGFBCPKDFFGEGKMIFCAJFBOEMFFABDGFMJCOCBEFLECFKAKLDDJNHDLBMFDMMCBHKNHGELCFCAGIDABNBABBDPAGCDIDEDINKEINBEKDEFCNBFAHHMLGNAJAAFUCGNSIEMHKOQQANLDLQDAECIFFAGFDUIAPEEADIJMJLCHGEBJPGIOGAOKBDBOJPCLJFFPBEAMIGAGGGRMGMGHHKMIGMKGBOHGMFIDCAGFLHIFKTDCCBAAHPADHDIEIJCBCFFEFAHGGJABGDNJADFLNIDADKDBBQELJDIKDJJNCBVDJCMDDAEOEHBBFHEFEFEAAHCEEHHHBNGADCEACACEDACMCEIMKIDHJAFJJJMEEOHJAAJAEKGEASFCDAPBHJDDNLDEDDCLACHAUBKNBIQDGEDFBFBMDCFAEEEBEDJAILGHGFPGBBMKAOCFHCFOMLFQNBPGUDDQBOFALINEBJKIBCCPFCEEKPJBEIRFLCEBBODBEEBEIAEDUCKFGFAFCFSSPUEBCMBUJFEORCQLEGPCIQUCEBBKCCBAIQFJAJQBTMLJSKQBQAHRBCABUCAACJMCAEKGBFQMGHICBEEABMBCDKHJBEEMJMBEBINODFDBJABCAALJCHBHEDEGENBIBQSHDIIGEFJDOGKBOJBANFLQFGSERHEKJSIAFJIKLIOSLDJ\""
      ],
      "metadata": {
        "id": "6dkm9YlpZKre"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ran_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1YxORrKZkpO",
        "outputId": "c25ec5de-70cf-4ce5-bc58-fffbcda7a4c7"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "B6RbKroEd6lj",
        "outputId": "5f5aa671-6694-43aa-aad1-230888aa0095"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'YFRVNLWGENCCIREHKCDQLFNGDNDHCEVYSSEQHFMPDENAHLGKLRGAHPAALFEKEXMEHHIHAMQDPSKPDCNGNLIKMDCORQPNNTQEENEFNEDQNHAFEWASRAMLWPWYPGNIDNCDCRNMAFENRRALAEHMGRYRKYNENYCKLRGKSNTWFILHNETILAKDMFLIMMYLIFIRGHEWNTDFQNYQLAMMSADDCTTQLCNCYVYDQWNLFKSMMDDFYRWLPOSHQYGYFTONRGCDLGPHQCLTETRVKOGWNQGWQKHWXSKIKQDCIWAWCNEMDKRKQVOYPVFHGNTFNMKFCFSQGVXWIAPVIAQHCTLWPRYDKAXMMMGBOTUFACTPFSGITWIWCFIZEHUDOSHSSEPYPARDFWWSYAWTXXVXFSMXMUAF'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bOzaQLnWd7YV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}